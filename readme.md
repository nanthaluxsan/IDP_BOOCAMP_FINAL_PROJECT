# Automated Detection of Abusive Content in Memes

Here, we used VGG16 and aToxigen-BERT model with LSTM to detect abusive content and provide confidence levels.

## Clone the repository

```bash
   git clone https://github.com/nanthaluxsan/IDP_BOOCAMP_FINAL_PROJECT
```

## Python environment

### Create a virtual environment

To create a Python virtual environment named `project-env`, run the following command:

```bash
python -m venv project-env
```

### Activate the virtual environment

windows

```bash
project-env\Scripts\activate
```

On macOS/Linux:

```bash
source project-env/bin/activate
```

### Install the dependencies

```bash
pip install -r requirements.txt
```

## React environment

React is used for front-end development. General CSS is used to style the page. The CSS files are located in the `CSS` folder, and the JavaScript files are in the `JS` folder.

### Navigation

```Base
cd Project-app
```

### dependencies

```Base
npm install
```

## Start the development server

Flask back-end

```Base
python app.py
```

React front-end

```Base
npm start
```

## Addition infromations

### Excluded Files

The following files are not used in the current implementation:

- Mila.py
- ocr_extraction.py
- toxigen_hate_bert.py

If you need to check the basic models or direct applications, refer to these files

### Toxigen Hate-BERT Model

For information about the Toxigen Hate-BERT model, refer to the links provided in the link.txt file within the repository.

## Further Details

For additional details about the project, you can access the Google Drive folder via the link below:

Google Drive - Further Details

<https://drive.google.com/drive/folders/1TRQmFgDId50vYTSy9PyPVqx50FmrAiG_?usp=sharing>
